set.seed(52550)
title_word_pairs %>%
  filter(n >= 40) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") + #use ?ggraph to see all the options
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "purple") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
  
  
  by_chapter_text_doc <-  by_chapter_text_doc[-grep("[0-9]", by_chapter_text_doc$word), ] 
by_chapter_text_doc <-  by_chapter_text_doc[-grep("chapter", by_chapter_text_doc$word), ] 
head(by_chapter_text_doc)


keyword_cors %>%
  filter(correlation > .6) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation, edge_width = correlation), edge_colour = "purple") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE,
                 point.padding = unit(0.2, "lines")) +
  theme_void()


rpackages <- c('papaja', 'knitr', 'citr', 'car','mlogit','readr','Amelia','caTools', 'dplyr', 'rpart', 'mlogit', 'nnet', 'ggplot2', 'caret', 'MASS', 'corrplot', 'gridExtra', 'randomForest', 'tidyverse', 'broom','moments')

for(p in packages){
  if(!require(p,character.only = TRUE)) install.packages(p, dependencies = TRUE)
  suppressMessages(library(p,character.only = TRUE, quietly = TRUE))
}


IF CONTAINS(UPPER([Strategic Priority ORG]),"ZDNU") THEN
    IF REGEXP_MATCH(UPPER([Strategic Priority ORG]),'^ZDNU\s\d{4}\s\-\s\S')
    THEN REGEXP_EXTRACT(UPPER([Strategic Priority ORG]),'^ZDNU\s\d{4}\s\-\s(.*)')
    ELSEIF REGEXP_MATCH(UPPER([Strategic Priority ORG]),'^ZDNU\s\d{4}\s\S')
    THEN REGEXP_EXTRACT(UPPER([Strategic Priority ORG]),'^ZDNU\s\d{4}\s(.*)')
    END
ELSE
[Strategic Priority ORG]
END

RemoveHTMLTags = function(htmlString) {
  return(gsub("<.*?>", " ", htmlString))}

RemoveWhiteSpaces = function(string) {
  return(gsub("\\s+", " ",string))}

#RemoveURLs = function(string) {
#  return(gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ",string))}

RemoveURLs = function(string) {
  return(gsub("(https?://(?:www[.]|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9][.][^\\s]{2,}|www[.][a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9][.][^\\s]{2,}|https?://(?:www[.]|(?!www))[a-zA-Z0-9]+[.][^\\s]{2,}|www[.][a-zA-Z0-9]+[.][^\\s]{2,}|https?://(?:go/)[a-zA-Z0-9]+|go/[a-zA-Z0-9]+)", " ",string, perl=TRUE))}

#RemoveUSAAgoURLs = function(string) {
#  return(gsub(" ?(f|ht)tp(s?)://go/(.*)[a-z]+(/?)", " ",string))}

RemoveEmails = function(string) {
  return(gsub("\\S+@\\S+", " ",string))}




Working with Python
Load the Python libraries and import the dataset from R.
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering
import matplotlib
matplotlib.use('Agg')
from matplotlib import pyplot as plt

sim_data = r.df 
#create distances
sim_dist = sch.linkage(sim_data, method='ward')
Create a dendogram of the variables.
##python chunk 
plt.figure()
plt.title("Hierarchical Clustering Dendogram")
plt.xlabel("Causal Variable")
plt.ylabel("Distance")
# create dendrogram
sch.dendrogram(sim_dist, #distance
              leaf_rotation=90., leaf_font_size=8.,
              labels = r.cluster_labels) #create tree
              
              
              
              plt.show()
              
              from sklearn import metrics
from scipy.cluster.hierarchy import fcluster

max_d = 49
for i in range(2, max_d):
  sil = metrics.silhouette_score(sim_data, fcluster(sim_dist, i, criterion='maxclust'), metric='euclidean')
  print(i, ":", sil)
  
  
  
  # Graphical Representation
import matplotlib.pyplot as plt
new_labels = km.labels_
fig,axes = plt.subplots(1,2,figsize=(16,8))
axes[0].scatter(X[:,0],X[:,1],c=y,cmap='gist_rainbow',edgecolor='k',s=150)
axes[1].scatter(X[:,0],X[:,1],c=new_labels,cmap='jet',edgecolor='k',s=150)
axes[0].set_xlabel('Sepal length',fontsize=18)
axes[0].set_ylabel('Sepal width',fontsize=18)
axes[1].set_xlabel('Sepal length',fontsize=18)
axes[1].set_ylabel('Sepal width',fontsize=18)
axes[0].tick_params(direction='in',length=10,width=5,colors='k',labelsize=20)
axes[1].tick_params(direction='in',length=10,width=5,colors='k',labelsize=20)
axes[0].set_title('Actual',fontsize=18)
axes[1].set_title('Predicted',fontsize=18)
plt.show()


# Dendrogram example
# create dendrogram
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering
#import sklearn.cluster
dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))
# create clusters
hc = AgglomerativeClustering(n_clusters=4,affinity ='euclidean',linkage ='ward')
# save clusters for chart
y_hc = hc.fit_predict(X)



from scipy.cluster.hierarchy import dendrogram,linkage
# generate the linkage matrix
Z = linkage(X,'average')
# set cut-off to 50
max_d = 7.08
# max_das in max_distance
plt.figure(figsize=(25,10))
plt.title('Iris Hierarchical Clustering Dendrogram')
plt.xlabel('Species')
plt.ylabel('distance')
dendrogram(Z,truncate_mode = 'lastp',# show only the last p merged clusters
           p= 10,# Try changing values of p
           leaf_rotation = 90.,# rotates the x axis labels
           leaf_font_size = 8.,# font size for the x axis labels
          )
plt.axhline(y=max_d , c='k')
plt.show


corr = AAA.corr()
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)
fig.colorbar(cax)
ticks = np.arange(0,len(AAA.columns),1)
ax.set_xticks(ticks)
plt.xticks(rotation=90)
ax.set_yticks(ticks)
ax.set_xticklabels(AAA.columns)
ax.set_yticklabels(AAA.columns)
plt.show()



# we can use following code to remove the features that have correlation over 0.6
# Create correlation matrix
corr_matrix = AAA.corr().abs()

# Select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Find index of feature columns with correlation greater than 0.60
to_drop = [column for column in upper.columns if any(upper[column] > 0.6)]

AAA_new1 = AAA.drop(to_drop, axis=1)

# Which volumn is dropped?
to_drop

# First, lets try k-means clustering
# Code to generate elbow graph to find the best value for number of cluters, k
from sklearn.cluster import KMeans 

sse = {} 
last_sse = 17592402.70373319 
for k in range(2, 15): 
    kmeans = KMeans(n_clusters=k, random_state=1234).fit(AAA_C) 
    AAA_C["clusters"] = kmeans.labels_ 
    #print(data["clusters"]) 
    sse[k] = kmeans.inertia_ 
    # Inertia: Sum of distances of samples to their closest cluster center 
    change_per = (last_sse-kmeans.inertia_)/last_sse*100 
    print ('At k= ',k,'The percentage of change in SSE is ',change_per,'%')
    last_sse = kmeans.inertia_ 
plt.figure() 
plt.plot(list(sse.keys()), list(sse.values())) 
plt.xlabel("Number of cluster") 
plt.ylabel("SSE") 
plt.show()

# alt code
# Using elbow method to find optimum k
distortions =[]
K = range(1,10)
for k in K:
    kmeanModel = KMeans(n_clusters=k).fit(X)
    kmeanModel.fit(X)
    distortions.append(kmeanModel.inertia_)
# Plot the elbow
plt.plot(K,distortions,'bx-')
plt.xlabel('k')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')
plt.show()




# Design a K means algorithm
# Design a model with best k=4
X = AAA_C.drop(['Absenteeism_Class'],axis=1)
y = AAA_C['Absenteeism_Class']

scaler = StandardScaler()
# scaler = MinMaxScaler()
scaler.fit(X)
X_scaled = scaler.transform(X)
# The MinMaxScaler shifts the data such that all features are exactly between 0 and 1
# The StandardScaler in scikit-learn ensures that for each feature the mean is 0 and variance is 1, bringing all features to the same magnitude
# However, this scaling does not ensure any particular minimum or maximum values for the features.

kmeans = KMeans(n_clusters=4, random_state=1234).fit(X_scaled)
pd.Series(kmeans.labels_).value_counts() 


y_pred = kmeans.predict(X_scaled)
mglearn.discrete_scatter(X_scaled[:, 6], X_scaled[:, 17], y_pred)
plt.legend(["cluster 0", "cluster 1", "cluster 2", "cluster 3"], loc='best')
plt.xlabel("Service Time")
plt.ylabel("Absenteeism time in hours")


# Lets plot the data. The cluster centers are stored in the cluster_centers_ attribute (plotted as triangles below)

mglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers='o')
mglearn.discrete_scatter(
    kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2],
    markers='^', markeredgewidth=2)
    
    
    
    # We can also use more or fewer clusters center i.e. clusters

fig, axes = plt.subplots(1, 2, figsize=(10, 5))

# using two cluster centers:
kmeans = KMeans(n_clusters=2)
kmeans.fit(X)
assignments = kmeans.labels_

mglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[0])

# using five cluster centers:
kmeans = KMeans(n_clusters=5)
kmeans.fit(X)
assignments = kmeans.labels_

mglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[1])







# Centroids 
kmeans.cluster_centers_
pd.DataFrame(data=kmeans.cluster_centers_,
             columns=X.columns)

# It is difficult to interpret the clusters due to the scaling on the data
# and because of the low accuracy score, these clusters are not the best split and not identifiable







from sklearn.cluster import DBSCAN
X = AAA_C.drop(['Absenteeism_Class'],axis=1)
y = AAA_C['Absenteeism_Class']
dbscan = DBSCAN()
clusters = dbscan.fit_predict(X)
print("Cluster memberships:\n{}".format(clusters))
# As you can see, all data points were assigned the label -1, which stands for noise
# This is a consequence of the default parameter settings for eps and min_samples


scaler = StandardScaler()
scaler.fit(X)
X_scaled = scaler.transform(X)

dbscan = DBSCAN(min_samples=6,eps=4)
clusters = dbscan.fit_predict(X_scaled)
print("Cluster memberships:\n{}".format(clusters))



pd.Series(clusters).value_counts() 



y_pred = dbscan.fit_predict(X_scaled)
mglearn.discrete_scatter(X_scaled[:, 6], X_scaled[:, 17], y_pred)
plt.legend(["Noise", "cluster 0", "cluster 1", "cluster 2", "cluster 3"], loc='best')
plt.xlabel("Service Time")
plt.ylabel("Absenteeism time in hours")




# While DBSCAN doesn't require setting the number of cluster explicitly, setting eps implicitly controls how many clusters will be found
# Finding a good setting for eps is sometimes easier after scaling the data using StandardScaler or MinMaxScaler.

X, y = make_moons(n_samples=200, noise=0.05, random_state=0)

# Rescale the data to zero mean and unit variance
scaler = StandardScaler()
scaler.fit(X)
X_scaled = scaler.transform(X)

dbscan = DBSCAN()
clusters = dbscan.fit_predict(X_scaled)
# plot the cluster assignments
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm2, s=60)
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")





# Compare the k-means, agglomerative and DBSCAN algorithms using ARI
from sklearn.metrics.cluster import adjusted_rand_score
X, y = make_moons(n_samples=200, noise=0.05, random_state=0)

# Rescale the data to zero mean and unit variance
scaler = StandardScaler()
scaler.fit(X)
X_scaled = scaler.transform(X)

fig, axes = plt.subplots(1, 4, figsize=(15, 3),
                         subplot_kw={'xticks': (), 'yticks': ()})

# make a list of algorithms to use
algorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),
              DBSCAN()]

# create a random cluster assignment for reference
random_state = np.random.RandomState(seed=0)
random_clusters = random_state.randint(low=0, high=2, size=len(X))

# plot random assignment
axes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,
                cmap=mglearn.cm3, s=60)
axes[0].set_title("Random assignment - ARI: {:.2f}".format(
        adjusted_rand_score(y, random_clusters)))

for ax, algorithm in zip(axes[1:], algorithms):
    # plot the cluster assignments and cluster centers
    clusters = algorithm.fit_predict(X_scaled)
    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters,
               cmap=mglearn.cm3, s=60)
    ax.set_title("{} - ARI: {:.2f}".format(algorithm.__class__.__name__,
                                           adjusted_rand_score(y, clusters)))
    
# Random cluster assignment have a ARI score of 0 and DBSCAN a score of 1. Hence, DBSCAN works best on this data.



import matplotlib.pyplot as plt
new_labels = km.labels_
fig,axes = plt.subplots(1,2,figsize=(16,8))
axes[0].scatter(X[:,0],X[:,1],c=y,cmap='gist_rainbow',edgecolor='k',s=150)
axes[1].scatter(X[:,0],X[:,1],c=new_labels,cmap='jet',edgecolor='k',s=150)
axes[0].set_xlabel('Sepal length',fontsize=18)
axes[0].set_ylabel('Sepal width',fontsize=18)
axes[1].set_xlabel('Sepal length',fontsize=18)
axes[1].set_ylabel('Sepal width',fontsize=18)
axes[0].tick_params(direction='in',length=10,width=5,colors='k',labelsize=20)
axes[1].tick_params(direction='in',length=10,width=5,colors='k',labelsize=20)
axes[0].set_title('Actual',fontsize=18)
axes[1].set_title('Predicted',fontsize=18)
plt.show()

kmeans = KMeans(
        n_clusters=3, init="k-means++",
        n_init=10,
        tol=1e-04, random_state=42
    )
kmeans.fit(X)
clusters=pd.DataFrame(X,columns=df.drop("ID",axis=1).columns)
clusters['label']=kmeans.labels_
polar=clusters.groupby("label").mean().reset_index()
polar=pd.melt(polar,id_vars=["label"])
fig4 = px.line_polar(polar, r="value", theta="variable", color="label", line_close=True,height=800,width=1400)
fig.show()



PCA


import pandas as pd
from sklearn import datasets
import numpy as np
iris = datasets.load_iris()
X = iris.data # Sepal and petal length and width
y = iris.target
IRIS =pd.DataFrame({'Sepal_Length':X[:,0],'Sepal_Width':X[:,1],'Petal_Length':X[:,2],'Petal_Width':X[:,3],'Species':y})

# Normalize data
from sklearn.preprocessing import scale
x_scaled = scale(X)

# Implement PCA
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
PC = pca.fit(x_scaled)


# PC Components
PC.components_


# Variance in components
PC.explained_variance_ratio_
# The 1st principal component captures 73% of the variance and the 2nd one captures 23%


# New features for all the instances
New_feats = pca.fit_transform(x_scaled)
New_feats

print("Original shape: {}".format(str(x_scaled.shape)))
print("Reduced shape: {}".format(str(New_feats.shape)))

# PCA Graphically
import matplotlib.pyplot as plt
principalDf = pd.DataFrame(data = New_feats, columns =['principal component 1','principal component 2'])
finalDf = pd.concat([principalDf,IRIS[['Species']]],axis =1)
fig = plt.figure(figsize=(8,8))
ax = fig.add_subplot(1,1,1)
ax.set_xlabel('Principal Component 1',fontsize=15)
ax.set_ylabel('Principal Component 2',fontsize=15)
ax.set_title('2 component PCA',fontsize=20)
targets = [0,1,2]
colors = ['r','g','b']
for target,color in zip(targets,colors):
    indicesToKeep=finalDf['Species']==target
    ax.scatter(finalDf.loc[indicesToKeep,'principal component 1'],finalDf.loc[indicesToKeep,'principal component 2'],c =color ,s =50)
ax.legend(['setosa','versicolor','virginica'])
ax.grid()



# One of the most common applications of PCA is visualizing high-dimensional data-sets
# For example, if we want to visualize the breast cancer data that has 30 features in a pair plot, there would be too many plots to look at
# We can create histograms (as below) of each of the features for the two classes, benign and malignant cancer
# But still this plot doesn't show us anything about the interactions between variables and how these relate to the classes
# Using PCA, we can capture the main interactions and get a slightly more complete picture (refer the code below)

fig, axes = plt.subplots(15, 2, figsize=(10, 20))
malignant = cancer.data[cancer.target == 0]
benign = cancer.data[cancer.target == 1]

ax = axes.ravel()

for i in range(30):
    _, bins = np.histogram(cancer.data[:, i], bins=50)
    ax[i].hist(malignant[:, i], bins=bins, color=mglearn.cm3(0), alpha=.5)
    ax[i].hist(benign[:, i], bins=bins, color=mglearn.cm3(2), alpha=.5)
    ax[i].set_title(cancer.feature_names[i])
    ax[i].set_yticks(())
ax[0].set_xlabel("Feature magnitude")
ax[0].set_ylabel("Frequency")
ax[0].legend(["malignant", "benign"], loc="best")
fig.tight_layout()



# Before we apply PCA, we scale out data so that each feature has unit variance using StandardScaler
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()

scaler = StandardScaler()
scaler.fit(cancer.data) # this is same as the scale method used above by the Prof.
X_scaled = scaler.transform(cancer.data)


from sklearn.decomposition import PCA
# keep the first two principal components of the data
pca = PCA(n_components=2)
# fit PCA model to beast cancer data
pca.fit(X_scaled)

# transform data onto the first two principal components
X_pca = pca.transform(X_scaled) # we need this because by default PCA only rotates and shifts the data, but keeps all PCs
print("Original shape: {}".format(str(X_scaled.shape)))
print("Reduced shape: {}".format(str(X_pca.shape)))


# plot first vs. second principal component, colored by class
plt.figure(figsize=(8, 8))
mglearn.discrete_scatter(X_pca[:, 0], X_pca[:, 1], cancer.target)
plt.legend(cancer.target_names, loc="best")
plt.gca().set_aspect("equal")
plt.xlabel("First principal component")
plt.ylabel("Second principal component")

# The downside of the PCA is that the two axes are not very easy to interpret
# The PCs correspond to the directions in the original data, so they are combinations of the original features


# The principal components are stored in the components_ attribute
# Each row corresponds to one PC, and they are sorted by their importance
#  The columns correspond to the original features attributes of the PCA (in this example "mean_radius","mean_texture" and so on)
print("PCA component shape: {}".format(pca.components_.shape))


print("PCA components:\n{}".format(pca.components_))


# We can also visualize the component's coefficients using a heat map
plt.matshow(pca.components_, cmap='viridis')
plt.yticks([0, 1], ["First component", "Second component"])
plt.colorbar()
plt.xticks(range(len(cancer.feature_names)),
           cancer.feature_names, rotation=60, ha='left')
plt.xlabel("Feature")
plt.ylabel("Principal components")

# You can see that in the 1st PC, all features have the same sign (positive)
# this means that there is a general correlation between all features

# The 2nd PC has mixed signs, and both of the components involve all of the 30 features






