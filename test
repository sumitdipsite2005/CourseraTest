set.seed(52550)
title_word_pairs %>%
  filter(n >= 40) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") + #use ?ggraph to see all the options
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "purple") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
  
  
  by_chapter_text_doc <-  by_chapter_text_doc[-grep("[0-9]", by_chapter_text_doc$word), ] 
by_chapter_text_doc <-  by_chapter_text_doc[-grep("chapter", by_chapter_text_doc$word), ] 
head(by_chapter_text_doc)


keyword_cors %>%
  filter(correlation > .6) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation, edge_width = correlation), edge_colour = "purple") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE,
                 point.padding = unit(0.2, "lines")) +
  theme_void()


rpackages <- c('papaja', 'knitr', 'citr', 'car','mlogit','readr','Amelia','caTools', 'dplyr', 'rpart', 'mlogit', 'nnet', 'ggplot2', 'caret', 'MASS', 'corrplot', 'gridExtra', 'randomForest', 'tidyverse', 'broom','moments')

for(p in packages){
  if(!require(p,character.only = TRUE)) install.packages(p, dependencies = TRUE)
  suppressMessages(library(p,character.only = TRUE, quietly = TRUE))
}


IF CONTAINS(UPPER([Strategic Priority ORG]),"ZDNU") THEN
    IF REGEXP_MATCH(UPPER([Strategic Priority ORG]),'^ZDNU\s\d{4}\s\-\s\S')
    THEN REGEXP_EXTRACT(UPPER([Strategic Priority ORG]),'^ZDNU\s\d{4}\s\-\s(.*)')
    ELSEIF REGEXP_MATCH(UPPER([Strategic Priority ORG]),'^ZDNU\s\d{4}\s\S')
    THEN REGEXP_EXTRACT(UPPER([Strategic Priority ORG]),'^ZDNU\s\d{4}\s(.*)')
    END
ELSE
[Strategic Priority ORG]
END

RemoveHTMLTags = function(htmlString) {
  return(gsub("<.*?>", " ", htmlString))}

RemoveWhiteSpaces = function(string) {
  return(gsub("\\s+", " ",string))}

#RemoveURLs = function(string) {
#  return(gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ",string))}

RemoveURLs = function(string) {
  return(gsub("(https?://(?:www[.]|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9][.][^\\s]{2,}|www[.][a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9][.][^\\s]{2,}|https?://(?:www[.]|(?!www))[a-zA-Z0-9]+[.][^\\s]{2,}|www[.][a-zA-Z0-9]+[.][^\\s]{2,}|https?://(?:go/)[a-zA-Z0-9]+|go/[a-zA-Z0-9]+)", " ",string, perl=TRUE))}

#RemoveUSAAgoURLs = function(string) {
#  return(gsub(" ?(f|ht)tp(s?)://go/(.*)[a-z]+(/?)", " ",string))}

RemoveEmails = function(string) {
  return(gsub("\\S+@\\S+", " ",string))}




Working with Python
Load the Python libraries and import the dataset from R.
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering
import matplotlib
matplotlib.use('Agg')
from matplotlib import pyplot as plt

sim_data = r.df 
#create distances
sim_dist = sch.linkage(sim_data, method='ward')
Create a dendogram of the variables.
##python chunk 
plt.figure()
plt.title("Hierarchical Clustering Dendogram")
plt.xlabel("Causal Variable")
plt.ylabel("Distance")
# create dendrogram
sch.dendrogram(sim_dist, #distance
              leaf_rotation=90., leaf_font_size=8.,
              labels = r.cluster_labels) #create tree
              
              
              
              plt.show()
              
              from sklearn import metrics
from scipy.cluster.hierarchy import fcluster

max_d = 49
for i in range(2, max_d):
  sil = metrics.silhouette_score(sim_data, fcluster(sim_dist, i, criterion='maxclust'), metric='euclidean')
  print(i, ":", sil)
  
  
  
  # Graphical Representation
import matplotlib.pyplot as plt
new_labels = km.labels_
fig,axes = plt.subplots(1,2,figsize=(16,8))
axes[0].scatter(X[:,0],X[:,1],c=y,cmap='gist_rainbow',edgecolor='k',s=150)
axes[1].scatter(X[:,0],X[:,1],c=new_labels,cmap='jet',edgecolor='k',s=150)
axes[0].set_xlabel('Sepal length',fontsize=18)
axes[0].set_ylabel('Sepal width',fontsize=18)
axes[1].set_xlabel('Sepal length',fontsize=18)
axes[1].set_ylabel('Sepal width',fontsize=18)
axes[0].tick_params(direction='in',length=10,width=5,colors='k',labelsize=20)
axes[1].tick_params(direction='in',length=10,width=5,colors='k',labelsize=20)
axes[0].set_title('Actual',fontsize=18)
axes[1].set_title('Predicted',fontsize=18)
plt.show()


# Dendrogram example
# create dendrogram
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering
#import sklearn.cluster
dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))
# create clusters
hc = AgglomerativeClustering(n_clusters=4,affinity ='euclidean',linkage ='ward')
# save clusters for chart
y_hc = hc.fit_predict(X)



from scipy.cluster.hierarchy import dendrogram,linkage
# generate the linkage matrix
Z = linkage(X,'average')
# set cut-off to 50
max_d = 7.08
# max_das in max_distance
plt.figure(figsize=(25,10))
plt.title('Iris Hierarchical Clustering Dendrogram')
plt.xlabel('Species')
plt.ylabel('distance')
dendrogram(Z,truncate_mode = 'lastp',# show only the last p merged clusters
           p= 10,# Try changing values of p
           leaf_rotation = 90.,# rotates the x axis labels
           leaf_font_size = 8.,# font size for the x axis labels
          )
plt.axhline(y=max_d , c='k')
plt.show


corr = AAA.corr()
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)
fig.colorbar(cax)
ticks = np.arange(0,len(AAA.columns),1)
ax.set_xticks(ticks)
plt.xticks(rotation=90)
ax.set_yticks(ticks)
ax.set_xticklabels(AAA.columns)
ax.set_yticklabels(AAA.columns)
plt.show()



# we can use following code to remove the features that have correlation over 0.6
# Create correlation matrix
corr_matrix = AAA.corr().abs()

# Select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Find index of feature columns with correlation greater than 0.60
to_drop = [column for column in upper.columns if any(upper[column] > 0.6)]

AAA_new1 = AAA.drop(to_drop, axis=1)

# Which volumn is dropped?
to_drop

# First, lets try k-means clustering
# Code to generate elbow graph to find the best value for number of cluters, k
from sklearn.cluster import KMeans 

sse = {} 
last_sse = 17592402.70373319 
for k in range(2, 15): 
    kmeans = KMeans(n_clusters=k, random_state=1234).fit(AAA_C) 
    AAA_C["clusters"] = kmeans.labels_ 
    #print(data["clusters"]) 
    sse[k] = kmeans.inertia_ 
    # Inertia: Sum of distances of samples to their closest cluster center 
    change_per = (last_sse-kmeans.inertia_)/last_sse*100 
    print ('At k= ',k,'The percentage of change in SSE is ',change_per,'%')
    last_sse = kmeans.inertia_ 
plt.figure() 
plt.plot(list(sse.keys()), list(sse.values())) 
plt.xlabel("Number of cluster") 
plt.ylabel("SSE") 
plt.show()

# alt code
# Using elbow method to find optimum k
distortions =[]
K = range(1,10)
for k in K:
    kmeanModel = KMeans(n_clusters=k).fit(X)
    kmeanModel.fit(X)
    distortions.append(kmeanModel.inertia_)
# Plot the elbow
plt.plot(K,distortions,'bx-')
plt.xlabel('k')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')
plt.show()




# Design a K means algorithm
# Design a model with best k=4
X = AAA_C.drop(['Absenteeism_Class'],axis=1)
y = AAA_C['Absenteeism_Class']

scaler = StandardScaler()
# scaler = MinMaxScaler()
scaler.fit(X)
X_scaled = scaler.transform(X)
# The MinMaxScaler shifts the data such that all features are exactly between 0 and 1
# The StandardScaler in scikit-learn ensures that for each feature the mean is 0 and variance is 1, bringing all features to the same magnitude
# However, this scaling does not ensure any particular minimum or maximum values for the features.

kmeans = KMeans(n_clusters=4, random_state=1234).fit(X_scaled)
pd.Series(kmeans.labels_).value_counts() 


y_pred = kmeans.predict(X_scaled)
mglearn.discrete_scatter(X_scaled[:, 6], X_scaled[:, 17], y_pred)
plt.legend(["cluster 0", "cluster 1", "cluster 2", "cluster 3"], loc='best')
plt.xlabel("Service Time")
plt.ylabel("Absenteeism time in hours")


# Lets plot the data. The cluster centers are stored in the cluster_centers_ attribute (plotted as triangles below)

mglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers='o')
mglearn.discrete_scatter(
    kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2],
    markers='^', markeredgewidth=2)
    
    
    
    # We can also use more or fewer clusters center i.e. clusters

fig, axes = plt.subplots(1, 2, figsize=(10, 5))

# using two cluster centers:
kmeans = KMeans(n_clusters=2)
kmeans.fit(X)
assignments = kmeans.labels_

mglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[0])

# using five cluster centers:
kmeans = KMeans(n_clusters=5)
kmeans.fit(X)
assignments = kmeans.labels_

mglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[1])







# Centroids 
kmeans.cluster_centers_
pd.DataFrame(data=kmeans.cluster_centers_,
             columns=X.columns)

# It is difficult to interpret the clusters due to the scaling on the data
# and because of the low accuracy score, these clusters are not the best split and not identifiable







from sklearn.cluster import DBSCAN
X = AAA_C.drop(['Absenteeism_Class'],axis=1)
y = AAA_C['Absenteeism_Class']
dbscan = DBSCAN()
clusters = dbscan.fit_predict(X)
print("Cluster memberships:\n{}".format(clusters))
# As you can see, all data points were assigned the label -1, which stands for noise
# This is a consequence of the default parameter settings for eps and min_samples


scaler = StandardScaler()
scaler.fit(X)
X_scaled = scaler.transform(X)

dbscan = DBSCAN(min_samples=6,eps=4)
clusters = dbscan.fit_predict(X_scaled)
print("Cluster memberships:\n{}".format(clusters))



pd.Series(clusters).value_counts() 



y_pred = dbscan.fit_predict(X_scaled)
mglearn.discrete_scatter(X_scaled[:, 6], X_scaled[:, 17], y_pred)
plt.legend(["Noise", "cluster 0", "cluster 1", "cluster 2", "cluster 3"], loc='best')
plt.xlabel("Service Time")
plt.ylabel("Absenteeism time in hours")




# While DBSCAN doesn't require setting the number of cluster explicitly, setting eps implicitly controls how many clusters will be found
# Finding a good setting for eps is sometimes easier after scaling the data using StandardScaler or MinMaxScaler.

X, y = make_moons(n_samples=200, noise=0.05, random_state=0)

# Rescale the data to zero mean and unit variance
scaler = StandardScaler()
scaler.fit(X)
X_scaled = scaler.transform(X)

dbscan = DBSCAN()
clusters = dbscan.fit_predict(X_scaled)
# plot the cluster assignments
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm2, s=60)
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")





# Compare the k-means, agglomerative and DBSCAN algorithms using ARI
from sklearn.metrics.cluster import adjusted_rand_score
X, y = make_moons(n_samples=200, noise=0.05, random_state=0)

# Rescale the data to zero mean and unit variance
scaler = StandardScaler()
scaler.fit(X)
X_scaled = scaler.transform(X)

fig, axes = plt.subplots(1, 4, figsize=(15, 3),
                         subplot_kw={'xticks': (), 'yticks': ()})

# make a list of algorithms to use
algorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),
              DBSCAN()]

# create a random cluster assignment for reference
random_state = np.random.RandomState(seed=0)
random_clusters = random_state.randint(low=0, high=2, size=len(X))

# plot random assignment
axes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,
                cmap=mglearn.cm3, s=60)
axes[0].set_title("Random assignment - ARI: {:.2f}".format(
        adjusted_rand_score(y, random_clusters)))

for ax, algorithm in zip(axes[1:], algorithms):
    # plot the cluster assignments and cluster centers
    clusters = algorithm.fit_predict(X_scaled)
    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters,
               cmap=mglearn.cm3, s=60)
    ax.set_title("{} - ARI: {:.2f}".format(algorithm.__class__.__name__,
                                           adjusted_rand_score(y, clusters)))
    
# Random cluster assignment have a ARI score of 0 and DBSCAN a score of 1. Hence, DBSCAN works best on this data.



import matplotlib.pyplot as plt
new_labels = km.labels_
fig,axes = plt.subplots(1,2,figsize=(16,8))
axes[0].scatter(X[:,0],X[:,1],c=y,cmap='gist_rainbow',edgecolor='k',s=150)
axes[1].scatter(X[:,0],X[:,1],c=new_labels,cmap='jet',edgecolor='k',s=150)
axes[0].set_xlabel('Sepal length',fontsize=18)
axes[0].set_ylabel('Sepal width',fontsize=18)
axes[1].set_xlabel('Sepal length',fontsize=18)
axes[1].set_ylabel('Sepal width',fontsize=18)
axes[0].tick_params(direction='in',length=10,width=5,colors='k',labelsize=20)
axes[1].tick_params(direction='in',length=10,width=5,colors='k',labelsize=20)
axes[0].set_title('Actual',fontsize=18)
axes[1].set_title('Predicted',fontsize=18)
plt.show()



